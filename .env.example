# OmniDev V3.0 Environment Configuration
# Copy this file to .env and fill in your values

# ============================================
# ACTIVE PROVIDER CONFIGURATION
# ============================================
# Choose your LLM provider and configure below
# Supported: openai, claude, gemini, grok, mistral, perplexity, meta, llama

OMNIDEV_PROVIDER_NAME=openai
OMNIDEV_PROVIDER_ENDPOINT=https://api.openai.com/v1/chat/completions
OMNIDEV_PROVIDER_API_KEY=your_api_key_here

# Model Configuration
OMNIDEV_MAX_TOKENS=2000
OMNIDEV_TEMPERATURE=0.7

# Application Settings
OMNIDEV_VERBOSE=false
OMNIDEV_LOG_LEVEL=INFO

# ============================================
# PROVIDER-SPECIFIC CONFIGURATIONS
# ============================================

# OpenAI (GPT-4, GPT-3.5, etc.)
# ü§ñ https://platform.openai.com/docs
# OMNIDEV_PROVIDER_NAME=openai
# OMNIDEV_PROVIDER_ENDPOINT=https://api.openai.com/v1/chat/completions
# OMNIDEV_PROVIDER_API_KEY=sk-...

# Anthropic Claude
# üß† https://docs.anthropic.com
# OMNIDEV_PROVIDER_NAME=claude
# OMNIDEV_PROVIDER_ENDPOINT=https://api.anthropic.com/v1/messages
# OMNIDEV_PROVIDER_API_KEY=sk-ant-...

# Google Gemini
# üíé https://ai.google.dev/docs
# OMNIDEV_PROVIDER_NAME=gemini
# OMNIDEV_PROVIDER_ENDPOINT=https://generativelanguage.googleapis.com/v1beta/models
# OMNIDEV_PROVIDER_API_KEY=...

# xAI Grok
# ‚ö° https://docs.x.ai
# OMNIDEV_PROVIDER_NAME=grok
# OMNIDEV_PROVIDER_ENDPOINT=https://api.x.ai/v1/chat/completions
# OMNIDEV_PROVIDER_API_KEY=xai-...

# Mistral AI
# üå¨Ô∏è https://docs.mistral.ai
# OMNIDEV_PROVIDER_NAME=mistral
# OMNIDEV_PROVIDER_ENDPOINT=https://api.mistral.ai/v1/chat/completions
# OMNIDEV_PROVIDER_API_KEY=...

# Perplexity AI
# üîç https://docs.perplexity.ai
# OMNIDEV_PROVIDER_NAME=perplexity
# OMNIDEV_PROVIDER_ENDPOINT=https://api.perplexity.ai/chat/completions
# OMNIDEV_PROVIDER_API_KEY=pplx-...

# Meta LLaMA (via Together AI or similar)
# ü¶ô https://llama.meta.com
# OMNIDEV_PROVIDER_NAME=meta
# OMNIDEV_PROVIDER_ENDPOINT=https://api.together.xyz/v1/chat/completions
# OMNIDEV_PROVIDER_API_KEY=...

# Local LLaMA (via Ollama or LM Studio)
# üè† https://ollama.ai/docs
# No API key required for local inference
# OMNIDEV_PROVIDER_NAME=llama
# OMNIDEV_PROVIDER_ENDPOINT=http://localhost:11434/v1/chat/completions
# OMNIDEV_PROVIDER_API_KEY=

# ============================================
# ADVANCED SETTINGS (Optional)
# ============================================
# OMNIDEV_TIMEOUT=30000
# OMNIDEV_RETRY_ATTEMPTS=3
